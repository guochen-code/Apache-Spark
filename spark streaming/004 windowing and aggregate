spark API automatically computes the sum across the batches
we do not need to code for it
spark streaming API takes care of it with the help of a state store

select(),filter(),map(),flatMap(),explode() are all stateless transformations
these operations will always work with the current micro-batch, has nothing to do with the history

the grouping and aggregation; windowing and joins transformations are stateful

side effects:
(1) stateless transformation: complete output mode not supported, you can only use append or update modes
(2) stateful transformation: excessive state causes out of memory

(1) why?
stateless queries will produce one or more records for each input record
10 records -> select statement -> 10 records
therefore remembering the outcome of the stateless queries are not efficient
hence, spark does not remeber the previous outcomes

(2) why?
the state is maintained in the memory of the spark executors. also backed up and checkpointed to the checkpoint location.
the checkpoined state is used for reloading the state after the failture or restart
at runtime, the state is cached in the executor memory
for this reason, state management is one of the critical design considerations
if you do not maintain the state properly and allow it to grow forever, you will run out of memory at some point
therefore, spark offers two types of stateful operations
(1) managed stateful operations 
spark will automatically clean the state entries when they are no longer needed
(2) unmanaged stateful operations
more flexible, allow to define your own custom state cleanup logic
however, it is evolving, only available in scala and java as of spark 3.0.0

when to use which
aggregations are two types:
(1) continuous aggregation
the loyalty aggregation is an ever-growing continuous aggregation, spark will never lean, not know when to clean
you need to define the logic
only save for 7 days. after that, the total accumulated points will be posted in the loyalty account for customers to redeem
next week, computation again restarts from zero, and you can again accumulate for the next 7 days.

(2) time-bound aggregations
so above case is a time-bound aggregations. all time-bound aggregations are the right candidate for spark to manage the state cleanup
continuous aggregation - apply a custom cleanup logic

********************* windowing aggregates
time-bound aggregates are also known as window aggregates. why? 
they are computed for a fixed time window and they are valid only until the window expires

in stream processing word, time windows are of two types.
(1) tumbling time window: a series of fixed-sized, non-overlapping time intervals
(2) sliding time window: an overlapping window

the aggregation window has nothing to do with the trigger time
the trigger is the time when you start processing data in a micro-batch

the time window is nothing but a grouping column !!!!!!!!!!!!!!!!

**************************************************************************************************************
trade_df=value_df.select("value.*") # trick, later on no need to use value.xxx, value.xxx for each column
                  .withColumn("CreatedTime", to_timestamp(col("CreatedTime"),"yyyy-MM-dd HH:mm:ss"))
                  .withColumn("Buy", expr("case when Type == 'BUY' then Amount else 0 end"))
                  .withColumn("Sell", expr("case when Type == 'SELL' then Amount else 0 end"))

window_agg_df=trade_df.groupBy(
                            window(col("CreatedTime"),"15 minute"))
                            .agg(sum("Buy").alias("TotalBuy"),
                            sum("Sell").alias("TotalSell"))
                                                       
'''
window_agg_df=trade_df.groupBy(col("BrokerCode"),
                            window(col("CreatedTime"),"15 minute")
                            )   # creating 15 minutes window for each broker
'''

******* if some records arrive late, later than their window, spark will take the older window from the state store, 
updated with the information of this new late comer 
give us a newly updated state of the same old window


**** however spark stream does not allow you to apply analytical functions and windowing regates inside a streaming query
the only windowing aggregate which you can get is the time-based windows and other standard aggregations

# how to fix
sink it to RDBMS and let your visualization team compute the running totals

summary:
1. tumbling window aggregates
2. what happends to late records: the late record will cause an update for the older window, and your sink should hanlde the updates !!!!!!!!
3. limitations of spark streaming: the workaround uses spark streaming for heay lifting and creates a separate app using batch processing capabilities for handling those requirements
                                   alternatively, you can sink the output to RDBMs and handle those simple things in RDBMs.




****************************************************************************** watermark
how to set an expiry date for the window state in the state store?
in spark, the expiry date for the window is known as watermark
decide your limit for the watermark is a business requirement

how long do you want to wait for your late-arriving records?
or when a late record no longer relevant?

.withWatermark("CreatedTime","30 minute") # 1st argument is the event time column; 2nd is watermark duration
.groupBy(window(col("CreatedTime"),"15 minute")) # must be called before groupBy
.agg(.....) # event time column should be the same, which you are going to use for windowing

so spark knows two things:
(1) ignore record if it arrives late by more than 30 minutes
(2) clean up the window state if it is older than 30 minutes

every micro-batch will update the watermark boundary
an indicative formula to arrive at the watermark boundary is given here:
*** Max(Event Time) - Watermark = Watermark Boundary
spark will clean the state store after committing the sink records and finalize the micro-batch
so the next micro-batch will start ignoring those late events for which we do not have a state

in summary:
(1) watermark is the key for state store cleanup. memory crisis without it
(2) events within the watermark is taken
(3) event outside the watermark may or may not be taken
    including an event to a window depends on the state-store. if the window is cleaned, spark will not take the event
    if the window state still exists, spark will consider it.
    
    
****************************************************************************** output mode
1. complete : because of completeness, cannot clean state store, even you give watermark. # use complete mode on time window aggregations with caution
  either complete mode or clean up, only can have one.
2. update == upsert
  spark does not support updating data files such as parquet or json, spark will always create a new file,
  so if you use update mode and sinking to file source, you will create duplicate records
  it should be used with sinks that support an upsert operation, such as cassandra database
3. append
  use this mode on aggregate queries will throw an analysis exception.
  this mode allowed only when spark knows the following record will not update or change in future
  because append output mode is desinged to output a record only once
  aggregate values might change in the future due to a late event
  but when you aggregate on event time and also apply a watermark, the scenario is changed
  because now spark knows when a window goes outside the watermark, it cannot change
  so spark allows you to use the append mode if you are aggregating on event time and applying a watermark
  
  # you will not get window because spark will hold this aggregate in the state store until the window state expires
  append mode will suppress the output of the windowing aggregates until they cross the watermark boundary
  this is a powerful feature, now you can use the append only sinks such a file sink
  because append mode will only emit the result once it is finalized, no further updates are required
  so you can sink the output to a file !!!!!!!!!!!!!!!!
  downside is that the result will be delayed at least by the watermark duration
  so if the delay is not a concern, you can use the append mode with a file sink
  
  if deplay is a concern, you should use the update mode with a sink, which supports an older record update 
  
  
  ****************************************************************************** sliding window
tumpling windows have 2 properties:
(1) fixed size
(2) not overlapping

sliding windows/hopping windows:

agg_df=senosr_df.withWatermark("CreatedTime","30 minute")
                .groupBy(col("SensorID"), window(col("CreatedTime"), "15 minute", "5 minute")) # 5 minute is the only change compared to tumpling window code!!!!!!!
                .agg(max("Reading").alias("MaxReading"))
  



