# first need to integrate spark with kafka
# go to spark-defaults.conf file
# copy and paste at the end of the file: spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0  #(copy from webpage)
#automatically download the required package and related dependencies from M aven
#alternatively, you set this up using python code in your script:
.config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0") # however, not recommended even it can

****************************************************************************************************** example - from kafka to files
spark = SparkSession.builder.appName("First Streaming Demo").master("local[3])
                    .config("spark.streaming.stopGracefullyOnShutdown","true")
                    .getOrCreate()
                    
logger = Log4j(spark)

kafka_df = spark.readStream.format("kafka")
                            .option("kafka.boostrap.servers","localhost:9092")
                            .option("subscribe","invoices")
                            .option("startingOffsets","earliest") # default is latest # the 
                            .load()
                            
kafka_df.printSchema()
# schema problem - look into the value which is the data we care about - it is in binary value
# need to assign a scheme 

from pyspark.sql.types import StructureType, StringType, LongType, DoubleType,IntegerType,ArrayType
from pyspark.functions import col

schema=StructuredType([
            StructuredField("InvoiceNumber", StringType()),
            StructuredField("CreatedTime", LongType()),
            .......... # DoubleType(), IntegerType(), ArrayType()
                            
value_df=kafka.select(col("value").cast("string")) # cast to a string because the value is in binary and the value contains json string

# assigning a schema to a json string is straightforward
value_df=kafka.select(from_json(col("value").cast("string"),schema)) # from_json takes 2 arguments (json,schema)
value_df=kafka.select(from_json(col("value").cast("string"),schema).alias("value")) # change the title name

explod_df = value_df.selectExpr("value.InvoiceNumner","value.CreatedTime",...."value.DeliveryAddress.PinCode",
                                "explode(value.InvoiceLineItems( as LineItem")
                                
                                
flattened_df = explode_df.withColumn("ItemCode", expr("LineItem.ItemCode"))
                         .withColumn("ItemDescription", expr("LineItem.ItemDescription")
                         .....
                         .withColumn("totalValue",expr("LineItem.TotalValue"))
                         .drop("LineItem")
                         
invoice_writer_query = flattened_df.writeStream.format("json")
                                                .queryName("Flattened Invoice Writer")
                                                .outputMode("append")
                                                .option("path","output")
                                                .option("checkpointLocation","chk-point-dir")
                                                .trigger(processingTime="1 minute")
                                                .start()

logger.info("Listening to Kafka")
invoice_writer_query.awaitTermination()

                      

****************************************************************************************************** example from kafka to kafka
# when working on transformations, you want to do it step by step and see the intermediate output dataframes
# because developing a series of transformations can be complicated
# however you can't show a streaming dataframe

# the trick is to change spark.readStream to spark.read !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# the read method is batch processing equivalent of the readStream
# still from kafka but is not going to loop into micro-batches
# you will read it once and do the transformations and stop
# you will be able to show your dataframe and debug your application
# once you done your experiments, change back to readStream

notification_df = value_df.select("value.InvoiceNumber",''''').withColumn("EarnLoyaltyPoints",expr("TotalAmount*0.2"))

# write back to kafka (!!! need a key/value pair !!!!) (if you want to send data to kafka, must have to columns named key and value) (need to serialize your key/value) 
# key column is a string and value column is a json record

kafka_target_df = notification_df.selectExpr("InvoiceNumber as key",
                                             """to_json(named_struct(
                                             'CustomerCardNo', customerCardNo,
                                             'TotalAmount', TotalAmount,
                                             'EarnedLoyaltyPoints', TotalAmount*0.2)) as value
                                             """)

notificaton_writer_query = notification_df.writeStream
                                          .queryName("Notification Writer")
                                          .format("kafka")
                                          .option("kafka.bootstrap.servers","localhost:9092")
                                          .option("topic","notifications")
                                          .outputMode("append")  # insert only
                                          .option("checkpointLocation","chk-point-dir")
                                          .start()
                                          
logger.info("Listening and Writing to Kafka")
notification_writer_query.awaitTermination()






