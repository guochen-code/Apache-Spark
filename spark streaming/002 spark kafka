# first need to integrate spark with kafka
# go to spark-defaults.conf file
# copy and paste at the end of the file: spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0  #(copy from webpage)
#automatically download the required package and related dependencies from M aven
#alternatively, you set this up using python code in your script:
.config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0") # however, not recommended even it can

****************************************************************************************************** example - from kafka to files
spark = SparkSession.builder.appName("First Streaming Demo").master("local[3])
                    .config("spark.streaming.stopGracefullyOnShutdown","true")
                    .getOrCreate()
                    
logger = Log4j(spark)

kafka_df = spark.readStream.format("kafka")
                            .option("kafka.boostrap.servers","localhost:9092")
                            .option("subscribe","invoices")
                            .option("startingOffsets","earliest") # default is latest # the 
                            .load()
                            
kafka_df.printSchema()
# schema problem - look into the value which is the data we care about - it is in binary value
# need to assign a scheme 

from pyspark.sql.types import StructureType, StringType, LongType, DoubleType,IntegerType,ArrayType
from pyspark.functions import col

schema=StructuredType([
            StructuredField("InvoiceNumber", StringType()),
            StructuredField("CreatedTime", LongType()),
            .......... # DoubleType(), IntegerType(), ArrayType()
                            
value_df=kafka.select(col("value").cast("string")) # cast to a string because the value is in binary and the value contains json string

# assigning a schema to a json string is straightforward
value_df=kafka.select(from_json(col("value").cast("string"),schema)) # from_json takes 2 arguments (json,schema)
value_df=kafka.select(from_json(col("value").cast("string"),schema).alias("value")) # change the title name

explod_df = value_df.selectExpr("value.InvoiceNumner","value.CreatedTime",...."value.DeliveryAddress.PinCode",
                                "explode(value.InvoiceLineItems( as LineItem")
                                
                                
flattened_df = explode_df.withColumn("ItemCode", expr("LineItem.ItemCode"))
                         .withColumn("ItemDescription", expr("LineItem.ItemDescription")
                         .....
                         .withColumn("totalValue",expr("LineItem.TotalValue"))
                         .drop("LineItem")
                         
invoice_writer_query = flattened_df.writeStream.format("json")
                                                .queryName("Flattened Invoice Writer")
                                                .outputMode("append")
                                                .option("path","output")
                                                .option("checkpointLocation","chk-point-dir")
                                                .trigger(processingTime="1 minute")
                                                .start()

logger.info("Listening to Kafka")
invoice_writer_query.awaitTermination()

                      

****************************************************************************************************** example from kafka to kafka
# when working on transformations, you want to do it step by step and see the intermediate output dataframes
# because developing a series of transformations can be complicated
# however you can't show a streaming dataframe

# the trick is to change spark.readStream to spark.read !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# the read method is batch processing equivalent of the readStream
# still from kafka but is not going to loop into micro-batches
# you will read it once and do the transformations and stop
# you will be able to show your dataframe and debug your application
# once you done your experiments, change back to readStream

notification_df = value_df.select("value.InvoiceNumber",''''').withColumn("EarnLoyaltyPoints",expr("TotalAmount*0.2"))

# write back to kafka (!!! need a key/value pair !!!!) (if you want to send data to kafka, must have to columns named key and value) (need to serialize your key/value) 
# key column is a string and value column is a json record

kafka_target_df = notification_df.selectExpr("InvoiceNumber as key",
                                             """to_json(named_struct(
                                             'CustomerCardNo', customerCardNo,
                                             'TotalAmount', TotalAmount,
                                             'EarnedLoyaltyPoints', TotalAmount*0.2)) as value
                                             """)

notificaton_writer_query = notification_df.writeStream
                                          .queryName("Notification Writer")
                                          .format("kafka")
                                          .option("kafka.bootstrap.servers","localhost:9092")
                                          .option("topic","notifications")
                                          .outputMode("append")  # insert only
                                          .option("checkpointLocation","chk-point-dir")
                                          .start()
                                          
logger.info("Listening and Writing to Kafka")
notification_writer_query.awaitTermination()

*************************************************************************************************** serialize & deserialize
# on the reading side.... deserialize....
1. string - cast()
2. csv - from_csv()
3. json - from_json()
3. AVRO - from_avro()

# on the writing side..... serialize....
to_json(struct(*)) / to_json(named_struct(*))
# struct() and named_struct() are used to create a dataframe structure
# the outcome is again used by the to_json for generating a JSON string
# the struct() method taks a list of selected columns
# the named_struct() allows you to rename the selected columns

*************************************************************************************************** AVRO sinks and source
two things working with Avro
(1) add avro dependency
(2) need avro schema (json records can be deserialized using spark schema, but the avro records would need the avro shcema)

# create kafka target dataframe, need to have a key and value
kafka_target_df = flattened.select(expr("InvoiceNumber as key"),
                                    to_avro(struct("*")).alias("value")) # struct"col1","col2")

invoice_writer_query = kafka_target_df.writeStream...... # same as above
# the to_avro() function requires an additional package named spark-avro
# add to spark-default.conf:
spark.jar.packages      org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-avro_2.12:3.0.0

# from_avro
kafka_source_df=spark.readStream.format("kafka").......load()
value_df=kafka_source_df.select(from_avro("value"),schema).alias("value"))
# you cannot use a dataframe schema to deserialize an Avro record
# the schema above is an Avro format schema string. The schema is defined using the Avro shcema definition language.
avroSchema=open('schema/invoice-items',mode='r').read()
value_df=kafka_source_df.select(from_avro("value"),avroSchema).alias("value"))

# calculate total value and rewards
rewards_df=value_df.filter("value.CustomerType=='PRIME'")
                   .groupBy("value.CustomerCardNo")
                   .agg(sum("value.TotalValue").alias("TotalPurchase"),
                   sum(expr("value.TotalValue*0.2").cast("integer")).alias(AggregatedRewards"))

kafka_target_df=rewards_df.select(expr("CustomerCardNo as key"),
                                  to_json(struct("TotalPurchase","AggregatedRewards")).alias("value"))
                                  
rewards_writer_query=kafka_target_df.writeStream......start()
rewards_writer_query.awaitTermination()




                                    


